{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf80b713",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "Model - \n",
    "lstm model (2 layers) using trigrams without word2vec, ran with 25 epochs\n",
    "\n",
    "Data -\n",
    "all taylor swift lyrics split in trigram sequences\n",
    "\n",
    "Results -\n",
    "pretty poor. approx 5% accuracy and nonsensical sentences. Accuracy did not seem to grow\n",
    "\n",
    "LSTM model takes forever (~5 mins each epoch) to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230a15e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, ngrams\n",
    "import contractions\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import keras.utils.np_utils as ku\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d79554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# misc global vars\n",
    "NGRAM_SIZE = 2 # note: we chose bigrams since the corpus is relatively small \n",
    "               # and higher-order ngrams are more often used for large (>1mil) corpora\n",
    "LINE_START = '<s>'\n",
    "LINE_END = '</s>'\n",
    "\n",
    "# embedding size note: this can be changed to whatever\n",
    "# we're using 100 to make the runtime less long\n",
    "EMBEDDINGS_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1144ac35",
   "metadata": {},
   "source": [
    "# data extraction stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3377e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_lst(file):\n",
    "    \"\"\"\n",
    "    Grabs the lyric section of the given csv and convert it to a string\n",
    "    Parameters:\n",
    "        file (CSV): file path where csv is located\n",
    "    Returns:\n",
    "        A list string representing all of the text section in the csv\n",
    "    \"\"\"\n",
    "    lyrics_df = pd.read_csv(file) \n",
    "#     lyrics_df = lyrics_df\n",
    "\n",
    "    lyrics_list = list(lyrics_df['lyrics'])\n",
    "\n",
    "\n",
    "    return lyrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f15c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_all_lyrics(lyric_list):\n",
    "    processed = []\n",
    "    for lyrics in lyric_list:\n",
    "#         print(lyrics)\n",
    "        processed.append(pre_process_text(lyrics))\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e38af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_line(line):\n",
    "    EMBED = 'embed'\n",
    "    word_list = word_tokenize(line)\n",
    "    \n",
    "   \n",
    "    #index = word_list.index('Lyrics')\n",
    "    #word_list = word_list[index + 1:]\n",
    "    \n",
    "    no_brackets_list = []\n",
    "    is_inside = False\n",
    "    punctuation_list = '''!()-[];:'\"\\,<>./?@#$%^&*_~”“’‘--...``'''''\n",
    "    \n",
    "    for word in word_list:\n",
    "        if word == '[':\n",
    "            is_inside = True\n",
    "        if word == ']':\n",
    "            is_inside = False\n",
    "        else:\n",
    "            if not is_inside:\n",
    "                if word not in punctuation_list:\n",
    "                    no_brackets_list.append(word.lower())\n",
    "\n",
    "    if len(no_brackets_list) > 0:\n",
    "        end_word = no_brackets_list[len(no_brackets_list)-1]   \n",
    "        if EMBED in end_word:\n",
    "#             print(end_word)\n",
    "            embed_location = end_word.index('embed') \n",
    "            # all lyric genius data comes with word 'Embed' at the end\n",
    "            substr = end_word[:embed_location]\n",
    "            if len(substr) > 0:\n",
    "                no_brackets_list[len(no_brackets_list)-1] = substr\n",
    "        \n",
    "        end_word = no_brackets_list[len(no_brackets_list)-1]  \n",
    "        new_end_word = ''\n",
    "        # also sometimes has a number before embed in last word\n",
    "        for c in end_word:\n",
    "            if c.isdigit():\n",
    "                break\n",
    "            else:\n",
    "                new_end_word = new_end_word + c\n",
    "        if len(end_word) > 0:\n",
    "            no_brackets_list[len(no_brackets_list)-1] = new_end_word\n",
    "       \n",
    "    #note: we are doing this so that when we try to generate full song lyrics later, we have a marker to decide when\n",
    "    # to move on to the next line\n",
    "    ret = [LINE_START]\n",
    "    for word in no_brackets_list:\n",
    "        if len(word) < 0:\n",
    "            break\n",
    "        #idk why this is like this \n",
    "        elif word == \"''\":\n",
    "            break\n",
    "        else:\n",
    "            ret.append(word)\n",
    "            \n",
    "    ret.append(LINE_END)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3748658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_text(lyrics):\n",
    "    \"\"\"\n",
    "    Preprocesses the text to add start and end tokens to each sentence\n",
    "    Parameters:\n",
    "        lst (list): list of lyrics \n",
    "    Returns:\n",
    "        List of List of words with start and end tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    lyrics = contractions.fix(lyrics)\n",
    "#     print(lyrics)\n",
    "    line_list = lyrics.split(\"\\n\")\n",
    "    line_list = line_list[1:] # removes the first line which contains meta info line artist name/song title etc\n",
    "    \n",
    "    ret = []\n",
    "    for line in line_list:\n",
    "        if not len(line) == 0:\n",
    "            to_be_appended = pre_process_line(line)\n",
    "            if len(to_be_appended) > 0:\n",
    "                ret.append(to_be_appended)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a5c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_songs(triple_list):\n",
    "    \"\"\"\n",
    "    Optional: flattens list of [songs[lines[words]]] (3 deep) to just one\n",
    "    long list of lines. This is used to make word vectors more smoothly\n",
    "    Parameters:\n",
    "        lst (list): list of lyrics \n",
    "    Returns:\n",
    "        List of List of words with start and end tokens\n",
    "    \"\"\"\n",
    "    long_list_lines = []\n",
    "    for song in triple_list:\n",
    "        for line in song:\n",
    "            long_list_lines.append(line)\n",
    "    return long_list_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2bbdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = csv_to_lst('taylorswift.csv')\n",
    "# print(lst)\n",
    "all_songs = pre_process_all_lyrics(lst)\n",
    "all_songs = flatten_songs(all_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dcb612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for song in all_songs:\n",
    "#     for line in song:\n",
    "#         print(line)\n",
    "print(all_songs[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb8bcfa",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf27448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3a175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(word_matrix):\n",
    "    \"\"\"\n",
    "    Trains a Word2Vec model on the corpus provided\n",
    "    Parameters:\n",
    "        word_matrix: a list of list of string representing words in sentences in \n",
    "                    a larger corpus\n",
    "                    eg:\n",
    "                    [[\"<s>\", \"this\", \"is\", \"an\", \"example\", \"</s>\"],\n",
    "                     [\"<s>\", \"this\", \"is\", \"also\", \"</s>\"]\n",
    "                    ]\n",
    "    Returns:\n",
    "        the trained Word2Vec model\n",
    "    \"\"\"\n",
    "    model = Word2Vec(sentences=word_matrix, vector_size= EMBEDDINGS_SIZE, window=5,min_count=1)\n",
    "    print('Vocab size {}'.format(len(model.wv.index_to_key)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce75394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = train_word2vec(all_songs)\n",
    "word_vectors = w2v_model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f09b51e",
   "metadata": {},
   "source": [
    "Note: might be interesting to make some graphs on song content!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf4bf76",
   "metadata": {},
   "source": [
    "## data visualization segue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ad0c5c",
   "metadata": {},
   "source": [
    "I was curious to see if there was any relation of lyrics between albums. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as keyed vector for easy use\n",
    "word_vectors.save('taylor_swift_wv.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42dd765",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_kv = KeyedVectors.load('taylor_swift_wv.kv')\n",
    "\n",
    "print('50 most commonly words:')\n",
    "print(word_kv.index_to_key[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab64c64",
   "metadata": {},
   "source": [
    "### some global vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_alpha = 0.7\n",
    "n_similar_words = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bef41b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_clusters(keys, kv, top_n_most_similar):\n",
    "    embedding_clusters = []\n",
    "    word_clusters = []\n",
    "    for word in keys:\n",
    "        embeddings = []\n",
    "        words = []\n",
    "        for similar_word, _ in kv.most_similar(word, topn=top_n_most_similar):\n",
    "            words.append(similar_word)\n",
    "            embeddings.append(kv[similar_word])\n",
    "        embedding_clusters.append(embeddings)\n",
    "        word_clusters.append(words)\n",
    "    return (word_clusters, embedding_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d6fe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_en_2d(embedding_clusters):\n",
    "    embedding_clusters_np = np.array(embedding_clusters)\n",
    "    n, m, k = embedding_clusters_np.shape\n",
    "    tsne_model_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
    "    embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters_np.reshape(n * m, k))).reshape(n, m, 2)\n",
    "    return embeddings_en_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e6ef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting stuff \n",
    "def tsne_plot_similar_words(title, labels, embedding_clusters, word_clusters, a, filename=None):\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
    "    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n",
    "        x = embeddings[:, 0]\n",
    "        y = embeddings[:, 1]\n",
    "        plt.scatter(x, y, c=color, alpha=a, label=label)\n",
    "        for i, word in enumerate(words):\n",
    "            plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n",
    "                         textcoords='offset points', ha='right', va='bottom', size=8)\n",
    "    plt.legend(loc=4)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    if filename:\n",
    "        plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba77b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarities(title, labels, kv, n_similar_words, a, filename):\n",
    "    '''plots similar words to the given labels\n",
    "    Parameters:\n",
    "    title -- title of the graph\n",
    "    labels -- list of words to find similar words to\n",
    "    kv -- keyed vectors to use aka whole dataset\n",
    "    n_similar_words -- number of similar words to find\n",
    "    a -- alpha value for graph\n",
    "    filename -- what to save the file as\n",
    "    '''\n",
    "    \n",
    "    cluster = embedding_clusters(labels, kv, n_similar_words)\n",
    "    en_2d = embeddings_en_2d(cluster[1])\n",
    "    \n",
    "    tsne_plot_similar_words(title, labels, en_2d, cluster[0], a, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68fe158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "275a3bb4",
   "metadata": {},
   "source": [
    "### by album name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e551868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not an exhaustive list, just the main ones\n",
    "# tragically speak now had to split into two words, which is prob not great for accuracy\n",
    "album_titles = ['speak', 'now', 'fearless', \n",
    "                'red', '1989', 'reputation', \n",
    "                'lover', 'folklore', 'evermore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93cc60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similarities('Words Clustered By Album Name',\n",
    "                 album_titles,\n",
    "                 word_kv,\n",
    "                 n_similar_words,\n",
    "                 graph_alpha,\n",
    "                 'taylor_swift_words_cluster_by_album.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b6f7e7",
   "metadata": {},
   "source": [
    "Not as clustered as I'd hoped :( but also not unclustered :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7da5f09",
   "metadata": {},
   "source": [
    "### by pronoun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a3f6ad",
   "metadata": {},
   "source": [
    "was also interested in pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a600498",
   "metadata": {},
   "outputs": [],
   "source": [
    "pronouns = ['i', 'me', 'you', 'he', 'she', 'they', 'we', 'us']\n",
    "\n",
    "plot_similarities('Words Clustered By Pronoun',\n",
    "                 pronouns,\n",
    "                 word_kv,\n",
    "                 n_similar_words,\n",
    "                 graph_alpha,\n",
    "                 'taylor_swift_words_cluster_by_pronoun.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8fe02e",
   "metadata": {},
   "source": [
    "### by posessive pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e91112",
   "metadata": {},
   "outputs": [],
   "source": [
    "posessive_pronouns = ['my', 'your', 'his', 'her', 'their', 'our']\n",
    "\n",
    "plot_similarities('Words Clustered By Posessive Pronoun',\n",
    "                 posessive_pronouns,\n",
    "                 word_kv,\n",
    "                 n_similar_words,\n",
    "                 graph_alpha,\n",
    "                 'taylor_swift_words_cluster_by_posessive_pronoun.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cae8cb2",
   "metadata": {},
   "source": [
    "# generating training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff73d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(all_songs)\n",
    "sequences = tokenizer.texts_to_sequences(all_songs)\n",
    "# print(flatten_genre[0:10])\n",
    "# print(word_embeddings[0:10])\n",
    "print('corpus len: ', len(sequences))\n",
    "# to_categorical needs this idk why\n",
    "# vocab_size = len(tokenizer.word_index) + 1\n",
    "# print('vocab size: ', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e623b10b",
   "metadata": {},
   "source": [
    "### split into ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75358f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngram_training_samples(word_embeddings, n) -> list:\n",
    "    '''\n",
    "    Takes the encoded data (list of lists) and \n",
    "    generates the training samples out of it.\n",
    "    Parameters:\n",
    "    word embeddigns = list\n",
    "    n = size of n gram\n",
    "    return: \n",
    "    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    '''\n",
    "    full_list = []\n",
    "    for we in word_embeddings:\n",
    "        full_list.extend(we)\n",
    "    ngram_list_tup = ngrams(full_list, n)\n",
    "    ngram_list = []\n",
    "    for tup in ngram_list_tup:\n",
    "        as_list = list(tup)\n",
    "        ngram_list.append(as_list)\n",
    "    return ngram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7fbc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_list = generate_ngram_training_samples(sequences, NGRAM_SIZE)\n",
    "print(ngrams_list[:10])\n",
    "print('num ngrams: ', len(ngrams_list))\n",
    "# print(ngrams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd6056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(ngrams_list):\n",
    "    '''\n",
    "    Splits n-grams into X predictor matrix and associated y label vector\n",
    "    Parameters:\n",
    "    ngrams_list = list of ngrams in form of [[0, 1, 2], ....]\n",
    "    return: \n",
    "    tuple (X, y)\n",
    "    '''\n",
    "    X = []\n",
    "    y = []\n",
    "    # splits ngrams into [a, b], [c]\n",
    "    for ng in ngrams_list:\n",
    "        x = []\n",
    "        for i in range(0, len(ng)):\n",
    "            if i < len(ng) - 1:\n",
    "                #predictor\n",
    "                x.append(ng[i])\n",
    "            else:\n",
    "                #label\n",
    "                y.append(ng[i])\n",
    "        X.append(x)\n",
    "    # note both need to be np.arrays to work with model\n",
    "    return (np.array(X), np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ae577",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred, y_label = get_X_y(ngrams_list)\n",
    "\n",
    "# spot check for correctness\n",
    "# for i in range(0, 10):\n",
    "#     print(X_pred[i], y_label[i])\n",
    "print('X shape: ', X_pred.shape)\n",
    "print('y shape: ', y_label.shape)\n",
    "\n",
    "print('y indexed at ?')\n",
    "copy = np.sort(y_label.copy())\n",
    "print(type(copy))\n",
    "print(copy[:10])\n",
    "print(copy[len(copy)-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f62108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_sequences_to_embeddings(word_vectors, tokenizer):\n",
    "    '''maps word's sequence to word's embedding\n",
    "    Parameters:\n",
    "        word_vectors word vector list from word2vec\n",
    "        tokenizer the tokenizer trained on the corpus\n",
    "    Returns:\n",
    "        dict mapping the word's sequence (from tokenizer) to it's embedding\n",
    "    '''\n",
    "    seq_to_embedding = dict()\n",
    "    for word in word_vectors.index_to_key:\n",
    "        embedding = word_vectors[word]\n",
    "        seq = tokenizer.word_index[word]\n",
    "        seq_to_embedding[seq] = embedding\n",
    "    \n",
    "    print('seq to embeddings map size: ', len(seq_to_embedding.keys()))\n",
    "    \n",
    "    return seq_to_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f621ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_to_embedding_map = map_sequences_to_embeddings(word_vectors, tokenizer)\n",
    "vocab_size = len(seq_to_embedding_map.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b1e7d1",
   "metadata": {},
   "source": [
    "### change bigrams to embeddings for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d7ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_embeddings(X: list, y: list, sequence_embeddings: dict) -> (list,list):\n",
    "    '''\n",
    "    turns X sequences and y sequences to embeddings\n",
    "    Parameters:\n",
    "        X - list of ngram sequences that are the predictors\n",
    "        y - list of sequences that are the associated labels\n",
    "        sequence_embeddings - maps sequences to w2v embeddings\n",
    "    Returns:\n",
    "        a tuple of (X_as_embeddings, y_as_embeddings)\n",
    "    '''\n",
    "    vocab_size = len(sequence_embeddings.keys())\n",
    "    \n",
    "    if len(X) != len(y):\n",
    "        print('error: something went wrong here -- X should be same len as y')\n",
    "    \n",
    "    X_embeddings = []\n",
    "    for ngram in X:\n",
    "        embedding = []\n",
    "        for gram in ngram:\n",
    "            embedding.extend(sequence_embeddings[gram])\n",
    "        X_embeddings.append(embedding)\n",
    "    \n",
    "    # use to_categorial to get one_hots\n",
    "    y_categorical_labels = ku.to_categorical(y) \n",
    "    \n",
    "    return (np.array(X_embeddings), y_categorical_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba819f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred and label initial inputs\n",
    "X_embedding_matrix, y_categorical_label = to_embeddings(X_pred,\n",
    "                                                         y_label,\n",
    "                                                         seq_to_embedding_map)\n",
    "\n",
    "# spot check\n",
    "print('x embeddings shape: ', X_embedding_matrix.shape)\n",
    "print('y label shape: ', y_categorical_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e736427",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_categorical_label[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e27bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_embedding_matrix[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232bfacf",
   "metadata": {},
   "source": [
    "# Create Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669800ac",
   "metadata": {},
   "source": [
    "### Baseline Statistical Model - Bigram Bag of Words\n",
    "We're doing this to see if using a Neural Network at all is overkill, since this corpus is relatively small. Code is adapted from HW2. Uses the raw bigrams as opposed to embeddings and also applies laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89176125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bigram_BOW_Model:\n",
    "    def __init__(self, tokenizer, bigrams, sequences):\n",
    "        self.vocab_size = len(tokenizer.word_index) + 1\n",
    "        bigram_matrix = self.bigram2matrix(bigrams)\n",
    "        flattened = np.concatenate(sequences)\n",
    "#         print(flattened)\n",
    "        word2count = Counter(flattened)\n",
    "        self.word2probability = self.calc_probability(bigram_matrix, \n",
    "                                                          word2count)\n",
    "#         print(bigram_matrix.shape)\n",
    "#         print(self.word2probability.shape)\n",
    "#         print(len(word2count))\n",
    "#         print(word2count)\n",
    "        pass\n",
    "    \n",
    "    def bigram2matrix(self, bigrams):\n",
    "        ''' creates a vocab_size*vocab_size matrix that counts the number\n",
    "        of times that a sequence appears after another sequence in the list\n",
    "        of ngrams\n",
    "        '''\n",
    "        M = np.zeros(shape=(self.vocab_size, self.vocab_size))\n",
    "        for pair in bigrams:\n",
    "#             print(pair)\n",
    "            w1 = pair[0]\n",
    "            w2 = pair[1]\n",
    "            M[w1][w2] += 1.0\n",
    "        return M\n",
    "\n",
    "    def calc_probability(self, bigram_matrix, word2count):\n",
    "        ''' creates a vocab_size*vocab_size matrix that tells the probability that\n",
    "        row sequence will be followed by col sequence.\n",
    "        '''   \n",
    "        print('bigram m ', bigram_matrix.shape)\n",
    "        print('unigram m ', len(word2count))\n",
    "        M = np.zeros(shape=(self.vocab_size, self.vocab_size))\n",
    "        for row in range(1, self.vocab_size):\n",
    "            for col in range(1, self.vocab_size):\n",
    "                M[row][col] = bigram_matrix[row][col] / (word2count[row])\n",
    "#                 print(M[row][col])\n",
    "#             print(np.sum(M[row]))\n",
    "        return M\n",
    "    \n",
    "    def predict(self, seed):\n",
    "        '''returns probability matrix of the given seed sequence'''\n",
    "        return [self.word2probability[seed]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2337f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ngrams_list[:10])\n",
    "# print(sequences)\n",
    "bow_model = Bigram_BOW_Model(tokenizer, ngrams_list, sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa62c3",
   "metadata": {},
   "source": [
    "### Feed-Forward Neural Network\n",
    "2-layer model with 150 hidden units per each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569e092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_FF_model(pred,\n",
    "                 label,\n",
    "                 hidden_units,\n",
    "                 vocab_size, \n",
    "                 num_epochs):\n",
    "    '''\n",
    "    Creates a model using a 2-layer Feedforward neural network\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=hidden_units, \n",
    "                    activation='relu', \n",
    "                    input_dim=pred.shape[1]))\n",
    "    model.add(Dense(units=hidden_units, \n",
    "                    activation='relu', \n",
    "                    input_dim=pred.shape[1]))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(units=vocab_size, \n",
    "                    activation='softmax'))\n",
    "    \n",
    "    # compile and fit\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(x=pred, \n",
    "                     y=label,\n",
    "                     epochs=num_epochs)\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fe8214",
   "metadata": {},
   "source": [
    "### LSTM Neural Network\n",
    "2-layer model with 50 hidden units per each layer (this is less than the feed-forward network because we found it doesn't really help to have more and also takes forever to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4188daf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM_model(pred,\n",
    "                     label,\n",
    "                     vocab_size,\n",
    "                     num_epochs):\n",
    "    '''\n",
    "    Creates a model using a 2-layer LSTM neural network\n",
    "    note: 50 hidden units for each layer\n",
    "    '''\n",
    "    #lstm expects input in the shape of (# samples, #timesteps, #features)\n",
    "    # we're using one feature per input \n",
    "    pred = pred.reshape((pred.shape[0], pred.shape[1], 1))\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(50, \n",
    "                   activation='relu',\n",
    "                   input_shape=(pred.shape[1], 1),\n",
    "                   return_sequences=True\n",
    "                  ))\n",
    "    model.add(LSTM(50, \n",
    "                   activation='relu'\n",
    "                  ))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(units=vocab_size, \n",
    "                    activation='softmax'))\n",
    "    \n",
    "    # compile and fit\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "    model.fit(x=pred, \n",
    "                     y=label,\n",
    "                     epochs=num_epochs)\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6823e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spot check\n",
    "print(X_embedding_matrix.shape)\n",
    "print(X_embedding_matrix[0].shape)\n",
    "print(X_embedding_matrix.shape[1])\n",
    "print(len(X_embedding_matrix[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43922e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = 200\n",
    "# plateaus after this\n",
    "num_epochs = 10\n",
    "ff_model = create_FF_model(X_embedding_matrix,\n",
    "                           y_categorical_label,\n",
    "                           hidden_units,\n",
    "                           vocab_size + 1,\n",
    "                           num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4088982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kinda plateau's after that\n",
    "num_epochs = 5\n",
    "lstm_model = create_LSTM_model(X_embedding_matrix,\n",
    "                            y_categorical_label,\n",
    "                            vocab_size + 1,\n",
    "                            num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43786d3f",
   "metadata": {},
   "source": [
    "# generate lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f64f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seed_embeddings(sequence_list, seq_to_embeddings_map, ngram_size):\n",
    "    '''from the list of seed sequences, maps them to their embeddings. \n",
    "    ensures that the embedding is ngram len.\n",
    "    NOTE: if there is no embedding for the given sequence, it will just return something random\n",
    "    '''\n",
    "    pred_size = ngram_size - 1\n",
    "    if len(sequence_list) > pred_size:\n",
    "        sequence_list = sequence_list[len(sequence_list) - pred_size:]\n",
    "    embedding_list = []\n",
    "    for seq in sequence_list:\n",
    "        embedding_list.extend(seq_to_embeddings_map[seq])\n",
    "\n",
    "    return np.array(embedding_list)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b48b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(tokenizer, \n",
    "                  model, \n",
    "                  seq_to_embedding_map, \n",
    "                  seed_text, \n",
    "                  num_lines_to_generate, \n",
    "                  ngram_size, \n",
    "                  isNN\n",
    "                 ):\n",
    "    words = []\n",
    "    # start off return set of words with the seed\n",
    "    words.extend(seed_text.split())\n",
    "    lines_generated_counter = 0\n",
    "    \n",
    "    while lines_generated_counter < num_lines_to_generate:\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        if len(token_list) == 0:\n",
    "            # invalid seed/seed doesn't exist in corpus -- choose a random one\n",
    "            random_seq = np.random.choice(np.arange(1, len(seq_to_embedding_map.keys())))\n",
    "            token_list = [random_seq]\n",
    "        \n",
    "        embeddings = get_seed_embeddings(token_list, seq_to_embedding_map, ngram_size)\n",
    "        \n",
    "        #basically are we using embeddings or not -- both NNs do, BOW doesn't\n",
    "        if isNN:\n",
    "            predicted = model.predict(np.reshape(embeddings, (-1, embeddings.shape[0])), verbose=0)\n",
    "        else:\n",
    "            predicted = model.predict(token_list[0])\n",
    "            \n",
    "\n",
    "        # ways of getting the top choice -- any are fine but we like np.random.choice the best\n",
    "        \n",
    "#         choice = np.argmax(predicted)\n",
    "        choice = np.random.choice(len(predicted[0]), p=predicted[0]) \n",
    "#         choice = sample(predicted[0])\n",
    "\n",
    "        word = tokenizer.index_word[choice]\n",
    "        \n",
    "        # so we stop eventually\n",
    "        if word == LINE_END:\n",
    "            lines_generated_counter += 1\n",
    "        \n",
    "        words.append(word)\n",
    "        seed_text = word # this is fine because we're running a bigram\n",
    "        \n",
    "    return format_lyrics(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ce7331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE: [TODO: fill in]\n",
    "def sample(preds, temperature=1):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b526cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_lyrics(words):\n",
    "    lyr = \"\"\n",
    "    # for testing\n",
    "    print(words)\n",
    "    print('----------')\n",
    "    for word in words:\n",
    "        if word == LINE_END:\n",
    "            # replace with newline char\n",
    "            lyr = lyr + \"\\n\"\n",
    "        else: \n",
    "            # add lyric\n",
    "            if word != LINE_START:\n",
    "                lyr = lyr + \" \" + word\n",
    "    return lyr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c028d70",
   "metadata": {},
   "source": [
    "# Compare Results\n",
    "Compares the resulting lyrics generated by each model using the same seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c15c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lyrics(seed, \n",
    "                             useBOW, \n",
    "                             useFF, \n",
    "                             useLSTM, \n",
    "                             n_lines):\n",
    "    '''Generates lyrics using the same seed and same number of lines for each model.\n",
    "    Parameters:\n",
    "    useBOW -- boolean flag for if we should use BOW model\n",
    "    useFF -- boolean flag for if we should use FF model\n",
    "    useLSTM -- boolean flag for if we should use LSTM model\n",
    "    seed -- seed text i.e.: 'i'\n",
    "    n_lines -- number of lines to generate\n",
    "    \n",
    "    '''\n",
    "    print('+++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    print('Seed: ', seed)\n",
    "    print('Num. Lines: ', n_lines)\n",
    "    print('+++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    \n",
    "    if useBOW:\n",
    "        lyrics = generate_text(tokenizer, \n",
    "                          bow_model,\n",
    "                          seq_to_embedding_map,\n",
    "                          seed, \n",
    "                          n_lines, \n",
    "                          NGRAM_SIZE,\n",
    "                          False)\n",
    "        print('==============================================')\n",
    "        print('Model Info: Laplace-Smoothed Bigram BOW Statistical Model')\n",
    "        print('----------------------')\n",
    "        print(lyrics)\n",
    "        print('==============================================')\n",
    "    \n",
    "    if useFF:\n",
    "        lyrics = generate_text(tokenizer, \n",
    "                          ff_model,\n",
    "                          seq_to_embedding_map,\n",
    "                          seed, \n",
    "                          n_lines, \n",
    "                          NGRAM_SIZE,\n",
    "                          True)\n",
    "        print('==============================================')\n",
    "        print('Model Info: Feed-Forward Neural Network')\n",
    "        print('----------------------')\n",
    "        print(lyrics)\n",
    "        print('==============================================')\n",
    "    if useLSTM:\n",
    "        lyrics = generate_text(tokenizer, \n",
    "                          lstm_model,\n",
    "                          seq_to_embedding_map,\n",
    "                          seed, \n",
    "                          n_lines, \n",
    "                          NGRAM_SIZE,\n",
    "                          True)\n",
    "        print('==============================================')\n",
    "        print('Model Info: LSTM Neural Network')\n",
    "        print('----------------------')\n",
    "        print(lyrics)\n",
    "        print('==============================================')\n",
    "    \n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dbc6eb",
   "metadata": {},
   "source": [
    "# Finally, some lyrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6840b505",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_to_generate = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7e1701",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_lyrics('you are', True, True, True, lines_to_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5387759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_lyrics('i am', True, True, True, lines_to_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb29a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_lyrics('this is', True, True, True, lines_to_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c9e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_lyrics('his', True, True, True, lines_to_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8553a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_lyrics('my', True, True, True, lines_to_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d7461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_lyrics('her', True, True, True, lines_to_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eebd654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add more here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5accb02e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
